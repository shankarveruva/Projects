{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date: 15/09/2019\n",
    "\n",
    "Version: 6.0\n",
    "\n",
    "Environment: Python 3.7.0 and Anaconda 2019.07 (64-bit)\n",
    "\n",
    "Libraries used:\n",
    "* pdfminer 20181108 (to extract data from PDF files)\n",
    "* nltk 3.4.4 (to tokenize and stem the data)\n",
    "* requests 2.22.0 (to access the links to download 200 PDFs)\n",
    "* re 2.2.1 (for regular expression, included in Anaconda Python 3.7)\n",
    "* pandas 0.24.2 (for data frame, included in Anaconda Python 3.7)  \n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "This assignment requires us to write Python code to extract the links from a PDF file (Group010.pdf) and then preprocess a set of published papers (200 files to be downloaded via links programatically) and convert the data into numerical representations (which are suitable for input into NLP AI systems, recommender-systems, information-retrieval algorithms, etc). \n",
    "\n",
    "We are required to do following tasks :\n",
    "\n",
    "1. Generate a sparse representation for Paper Bodies (i.e. paper text without Title, Authors, Abstract and References). The sparse representation consists of two files:\n",
    "\n",
    "    a. Vocabulary index file<br>\n",
    "    b. Sparse count vectors file<br><br>\n",
    "\n",
    "2. Generate a CSV file (stats.csv) containing three columns:\n",
    "\n",
    "    a. Top 10 most frequent terms appearing in all Titles<br>\n",
    "    b. Top 10 most frequent Authors<br>\n",
    "    c. Top 10 most frequent terms appearing in all Abstracts<br>\n",
    "\n",
    "\n",
    "More details will be provided in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In memory stream for text I/O\n",
    "import io\n",
    "from io import StringIO\n",
    "\n",
    "# To extract information from PDF documents\n",
    "import pdfminer\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "# For tokenizing and stemming\n",
    "import nltk.data\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import *\n",
    "\n",
    "# To access links and download data\n",
    "import requests\n",
    "\n",
    "# For usage of regular expressions\n",
    "import re\n",
    "\n",
    "# For statistic generation\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### readTextFromPdf\n",
    "\n",
    "The `readTextFromPdf` function is used to read the data from a PDF file. We are using in-built functions of the pdfminer library to read the content of the PDF file. This functions returns the text in form of a string.\n",
    "\n",
    "\n",
    "We are passing two arguments to this function:\n",
    "\n",
    "- path - location of the file to be read\n",
    "\n",
    "- pages - always set to none, can be changed if there's a need to access data page wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read the data from a PDF file\n",
    "\n",
    "def readTextFromPdf(path, pages=None):\n",
    "    if not pages:\n",
    "        pagenums = set()\n",
    "    else:\n",
    "        pagenums = set(pages)\n",
    "        \n",
    "    resourceManager = PDFResourceManager()                        # Repository of shared resources\n",
    "    fakeFileHandle = io.StringIO()                                # Text I/O using in-memory buffer\n",
    "    converter = TextConverter(resourceManager, fakeFileHandle, laparams=LAParams())      # Convert the text through interpreter\n",
    "    pageInterpreter = PDFPageInterpreter(resourceManager, converter)\n",
    " \n",
    "    with open(path, 'rb') as fileHandle:                          # Opening the PDF file as fileHandle\n",
    "        for page in PDFPage.get_pages(fileHandle, pagenums):      # Go through each page if page exists\n",
    "            pageInterpreter.process_page(page)\n",
    " \n",
    "        text = fakeFileHandle.getvalue()                          # Fetching the data from PDF file\n",
    " \n",
    "    # close open handles\n",
    "    converter.close()\n",
    "    fakeFileHandle.close()\n",
    " \n",
    "    # If there's some text in the file, pass text or else pass None\n",
    "    if text:\n",
    "        return text\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### getFilenameAndUrl\n",
    "\n",
    "The `getFilenameAndUrl` function is used to extract the content from `Group010.pdf` file. We do this by removing `filename` and `url` strings, squeezing multiple new line characters `\\n` into one `\\n` and using the regex findall function to read pdf filenames with the regex pattern `(PP\\d{4})\\.pdf` and it's URL with the regex pattern `(http.*?)\\n`. After we extract all the filenames and their links, we create a list of tuples, fileLinkPair. At the end, this function returns the fileLinkPair.\n",
    "\n",
    "We are passing 1 argument to this function:\n",
    "\n",
    "- text - the content that is to be passed so that this function can extract the required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To extract filename and URL from the text that was read through readTextFromPdf function\n",
    "\n",
    "def getFilenameAndUrl(text):\n",
    "    text = re.sub(r'filename|url','', text)         # Removing the phrase filename or url\n",
    "    text = re.sub(r'\\n+','\\n', text)                # Substituting multiple new line characters with one new line character\n",
    "    file = re.findall(r'(PP\\d{4})\\.pdf', text)      # Finding all file names (200)\n",
    "    link = re.findall(r'(http.*?)\\n', text)         # Finding all URLs (200)\n",
    "    \n",
    "    fileLinkPair = []\n",
    "    for i in range(len(file)):                      # Creating a file link pair list of file name and link tuples\n",
    "        fileLinkPair.append((file[i].strip(), link[i].strip()))\n",
    "\n",
    "    return fileLinkPair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stemWords\n",
    "\n",
    "The `stemWords` fuction allows us to pass a list of words as an argument and perform stemming on it. Stemming is performed using `PorterStemmer`. This is one of the last steps of processing. In this function, there are four sub levels of checks as stemming on any word structure results in output that is in lowercase. The checks are as below:\n",
    "\n",
    "- As per specifications, only unigrams are to be stemmed. So, if the word is a bigram, instead of stemming it, we simply append to the list that is to be returned at the end of this function.\n",
    "\n",
    "- If the first letter of the word is in uppercase and rest of the letters are in lowercase, then we stem that word but later restore the same structure of that word before appending it to the list that is returned at the end of this function.\n",
    "\n",
    "- Just as above step, for a word that is in upper case, we stem that word and then restore the same structure of that word before appending it to the list that is returned at the end of this function.\n",
    "\n",
    "- Lastly, if the word is in lowercase, we simply stem it and append it to the final list that is to be returned at the end of this function.\n",
    "\n",
    "\n",
    "We are passing 1 argument to this function:\n",
    "\n",
    "- listOfWords - list of words that are to be stemmed using porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To stem the words (In each case, the pattern is restored after stemming)\n",
    "\n",
    "def stemWords(listOfWords):\n",
    "    stemList=[]\n",
    "    ps = PorterStemmer()\n",
    "        \n",
    "    # Stemming performs lower case by default. So, we set four conditions :\n",
    "    for word in listOfWords:\n",
    "        \n",
    "        # If bigram, simply append, don't stem\n",
    "        if '__' in word:\n",
    "            stemList.append(word)\n",
    "        \n",
    "        # If the first letter is uppercase and next letter isin lower case\n",
    "        elif word[0].isupper() and word[1].islower():\n",
    "            word = ps.stem(word)\n",
    "            word = word.replace(word[0], word[0].upper(), 1)\n",
    "            stemList.append(word)\n",
    "            \n",
    "        # If the whole word is uppercase\n",
    "        elif word.isupper():\n",
    "            word = ps.stem(word)\n",
    "            word = word.upper()\n",
    "            stemList.append(word) \n",
    "        \n",
    "        # For any other case\n",
    "        else:    \n",
    "            stemList.append(ps.stem(word))\n",
    "\n",
    "    return stemList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tokenizer\n",
    "\n",
    "The `tokenizer` function is used to tokenize the text i.e. splitting a sentence into tokens. Firstly, we use `[A-Za-z]\\w+(?:[-'?]\\w+)?` pattern for the `RegexpTokenizer` and then using that tokenizer, we tokenize the text and form unnigram tokens. \n",
    "\n",
    "We are passing 1 argument to this function:\n",
    "\n",
    "- text - sentences that are to be tokenized following a regex pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To tokenize sentences\n",
    "\n",
    "def tokenizer(text):\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\")\n",
    "    unigram_tokens = tokenizer.tokenize(text)\n",
    "        \n",
    "    return unigram_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extracting stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are extracting all the context-independent stop words from `stopwords_en.txt` file and creating a list out of all the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading stop words from given text file and storing them into a list\n",
    "\n",
    "stopWords = []\n",
    "\n",
    "stopWordsFile = open('stopwords_en.txt', 'r')\n",
    "\n",
    "for word in stopWordsFile:\n",
    "    stopWords.append(word.strip('\\n'))\n",
    "\n",
    "stopWordsFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extracting filenames as well as links from _'Group010.pdf'_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above created `readTextFromPdf` function, we read the content of the file `Group010.pdf` and extract the file - link pair using the function `getFilenameUrl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading text from Group010.pdf \n",
    "fileContent = readTextFromPdf('Group010.pdf')\n",
    "\n",
    "# Extracting file and link pair to download the data\n",
    "fileLinkPair = getFilenameAndUrl(fileContent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Downloading 200 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting the filenames and their URL, we download all the files using the `requests` library provided by Python, which helps in making simple HTTP requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download 200 pdf files from links\n",
    "\n",
    "for i in range(len(fileLinkPair)):\n",
    "    url = fileLinkPair[i][1]\n",
    "    file = requests.get(url)\n",
    "    fileName = fileLinkPair[i][0] + '.pdf'\n",
    "    with open(fileName, 'wb') as fileHandle:\n",
    "        fileHandle.write(file.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initializing variables \n",
    "\n",
    "Here, we have initialized few variables that would be used in the later stages of this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary for Paper Body\n",
    "paperBodyDict = {}\n",
    "\n",
    "# Creating a dictionary for authors\n",
    "authorDict = {}\n",
    "\n",
    "# Creating an empty string for titles\n",
    "titles = \"\"\n",
    "\n",
    "# Initializing empty dictionary to store top title words and their frequency\n",
    "titleDict = {}\n",
    "\n",
    "# Creating an empty string for abstract\n",
    "abstracts = \"\"\n",
    "\n",
    "# Initializing empty dictionary to store top abstract words and their frequency \n",
    "abstractDict = {}\n",
    "\n",
    "# Initializing empty list to store all tokens\n",
    "fullTokenList = []\n",
    "\n",
    "# Initializing empty list to store bigram words\n",
    "bigramWordList = []\n",
    "\n",
    "# Initializing empty list for step 12.2\n",
    "tokenWordList = []\n",
    "\n",
    "# Initializing empty list to store vocabulary information\n",
    "vocabList = []        \n",
    "\n",
    "# Initializing empty dictionary to store words after removing stop words\n",
    "dictWithoutStopWords = {}\n",
    "\n",
    "# Initializing empty dictionary to store words after removing words with length less than 3\n",
    "dictWithoutSmallTokens = {}\n",
    "\n",
    "# Initializing empty dictionary to store final set of words\n",
    "finalDictionary = {}\n",
    "\n",
    "# Initializing empty vocab list to store vocabulary information after stemming\n",
    "vocabularyList = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Extracting paper body, author names, titles and abstract from each PDF file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are creating four separate dictionaries, each having content from different part of the pdf file. \n",
    "\n",
    "1. We extracted `Paper Body` using the pattern `1 Paper Body(.*?)2 References`. We did this using re.search and group command.\n",
    "\n",
    "2. We extracted `Authors` using the pattern `Authored by:(.*?)Abstract`. We did this using re.search and group command. After this, we create a dictionary with auther name as it's key and frequency of the name's occurences in all the pdf files as it's value.\n",
    "\n",
    "3. We extracted `Title` using the pattern `^(.*?)Authored by:`. We did this using re.search and group command. After this, we create a dictionary with title words as it's key and frequency of the word's occurences in all the pdf files as it's value.\n",
    "\n",
    "4. We extracted `Abstract` using the pattern `Abstract(.*?)1 Paper Body`. We did this using re.search and group command. After this, we create a dictionary with abstract words as it's key and frequency of the word's occurences in all the pdf files as it's value.\n",
    "\n",
    "**NOTE :**\n",
    "Before creating dictionaries, we are correcting the pattern of the text that is read from PDF file otherwise these wrong patterns might lead to incorrect vocabulary count and count vectors. For example, we have found patterns such as `ﬃ` and `ﬀ` and have replaced those patterns with `ffi` and `ff` respectively. This will help us in getting the correct format of the tokens and their actual frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To access each of the 200 files\n",
    "\n",
    "for i in range(len(fileLinkPair)):\n",
    "    fileName = fileLinkPair[i][0] + '.pdf'\n",
    "    readText = readTextFromPdf(fileName)\n",
    "    \n",
    "    # Extracting Paper Body\n",
    "    paperBody = re.search('1 Paper Body(.*?)2 References', readText, re.DOTALL) \n",
    "    paperBody = paperBody.group(1).strip()\n",
    "    \n",
    "    # Removing weird patterns in paper body\n",
    "    paperBody = re.sub('ﬃ', 'ffi', paperBody)\n",
    "    paperBody = re.sub('ﬀ', 'ff', paperBody)\n",
    "    paperBody = re.sub('ﬁ', 'fi', paperBody)\n",
    "    paperBody = re.sub('ﬄ', 'ffl', paperBody)\n",
    "    paperBody = re.sub('ﬂ', 'fl', paperBody)\n",
    "    \n",
    "    # Storing paperBody into dictionary with filename as key\n",
    "    paperBodyDict[fileLinkPair[i][0]] = paperBody\n",
    "    \n",
    "    \n",
    "    # Extracting author names\n",
    "    authors = re.search('Authored by:(.*?)Abstract', readText, re.DOTALL)   \n",
    "    authors = authors.group(1).strip()\n",
    "    authors = re.sub('\\n+', '\\n', authors)\n",
    "    \n",
    "    # Removing weird patterns in author names\n",
    "    authors = re.sub('ﬃ', 'ffi', authors)\n",
    "    authors = re.sub('ﬀ', 'ff', authors)\n",
    "    authors = re.sub('ﬁ', 'fi', authors)\n",
    "    authors = re.sub('ﬄ', 'ffl', authors)\n",
    "    authors = re.sub('ﬂ', 'fl', authors)\n",
    "    \n",
    "    # Creating an author name list for a file\n",
    "    authorList = authors.split('\\n')                                        \n",
    "    \n",
    "    # Updating the author dictionary\n",
    "    for name in authorList:                                                 \n",
    "        if name in authorDict.keys():\n",
    "            authorDict[name] += 1                # If the name exists in the dict, increment value by 1\n",
    "        else:\n",
    "            authorDict[name] = 1                 # If the name doesn't exists in the dict, assign value as 1\n",
    "            \n",
    "      \n",
    "    # Extracting titles\n",
    "    title = re.search('^(.*?)Authored by:', readText, re.DOTALL)   \n",
    "    title = title.group(1).strip()\n",
    "    title = re.sub('\\n+', ' ', title)\n",
    "    \n",
    "    # Removing weird patterns in title\n",
    "    title = re.sub('ﬃ', 'ffi', title)\n",
    "    title = re.sub('ﬀ', 'ff', title)\n",
    "    title = re.sub('ﬁ', 'fi', title)\n",
    "    title = re.sub('ﬄ', 'ffl', title)\n",
    "    title = re.sub('ﬂ', 'fl', title)\n",
    "    \n",
    "    # Storing all titles in form of a string\n",
    "    titles = titles + ' ' + title\n",
    "\n",
    "        \n",
    "    # Extracting abstract\n",
    "    abstract = re.search('Abstract(.*?)1 Paper Body', readText, re.DOTALL)   \n",
    "    abstract = abstract.group(1).strip()\n",
    "    \n",
    "    # Removing weird patterns in abstract\n",
    "    abstract = re.sub('ﬃ', 'ffi', abstract)\n",
    "    abstract = re.sub('ﬀ', 'ff', abstract)\n",
    "    abstract = re.sub('ﬁ', 'fi', abstract)\n",
    "    abstract = re.sub('ﬄ', 'ffl', abstract)\n",
    "    abstract = re.sub('ﬂ', 'fl', abstract)\n",
    "    \n",
    "    # Storing all abstracts in form of a string\n",
    "    abstracts = abstracts + ' ' + abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sentence segmentation and normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are breaking the whole Paper Body content of each file into sentences using `Punkt Sentence Tokenizer` which we are loading from the NLTK package and normalizing the content as per assignment specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To break string into sentences and then normalize the content\n",
    "sentenceDetector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "for fileName, body in paperBodyDict.items():\n",
    "    sentences = sentenceDetector.tokenize(body)     # Breaking string into sentences\n",
    "    normalizedData = []\n",
    "    \n",
    "    for sentence in sentences:                      # To access each sentence from the list of sentences\n",
    "        combinedData = \"\"\n",
    "        sentence = re.sub('-\\n', '', sentence)      # Substitute '-\\n' with nothing to join words that were separated in file\n",
    "        sentence = re.sub('\\n', ' ', sentence)      # Substituting new line character with space\n",
    "        wordList = sentence.split()                 # Splitting sentence into words\n",
    "        wordList[0] = wordList[0].lower()           # Normalizing only first word and leaving middle words as they are\n",
    "        combinedData += ' '.join(wordList)\n",
    "        normalizedData.append(combinedData)\n",
    "        \n",
    "    paperBodyDict[fileName] = normalizedData        # Storing normalized data in a dictionary with it's filename as key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of breaking down a character sequence into pieces is known as tokenization. Here, we are breaking down the sentences into tokens using the `tokenizer` function defined above, generating a list of tokens and assigning it back to it's respective key position in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To tokenize the normalized data\n",
    "for fileName, body in paperBodyDict.items():\n",
    "    tokenList = []\n",
    "    \n",
    "    for sentence in body:\n",
    "        token = tokenizer(sentence)       # Tokenize each sentence in the paper body from paperBodyDict dictionary\n",
    "        tokenList.extend(token)           # Extending the tokenList for a particular paperbody under one file\n",
    "        \n",
    "    fullTokenList.extend(tokenList)       # Extending the tokenList for all the files\n",
    "    paperBodyDict[fileName] = tokenList   # Storing token list in a dictionary with it's filename as key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Generating bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can form n-grams by extracting a continuous sequence of `n` words from a given sentence. By picking `n=2` we can form bigrams.\n",
    "\n",
    "Firstly, we remove the context dependent stopwords only for the purpose of generating meaningful bigrams and then we can extract a list of bigrams using the function `ngrams()`. We need to pass a list of words and `n=2` as arguments for this function.\n",
    "\n",
    "Then we use `FreqDist()` function provided by the NLTK package which helps us to compute the dictribution directly from a set of tokens.\n",
    "\n",
    "FInally, we are extracting the the top 200 bigrams based on the highest frequency using the `most_common()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing context independent stopwords for generating bigrams\n",
    "tokenListWithoutStopwords = [word for word in fullTokenList if word not in stopWords]\n",
    "\n",
    "# Generating bigrams \n",
    "bigrams = ngrams(tokenListWithoutStopwords, n = 2)\n",
    "\n",
    "# Finding the frequency of the bigrams and sorting them in the highest frequency\n",
    "fdbigram = FreqDist(bigrams) \n",
    "\n",
    "# Taking the top 200 bigrams\n",
    "bigramList = fdbigram.most_common(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting only the bigram tokens and then joining it based on `'__'` and tokenizing it using the MWETokenizer. Lastly, we retokenize everything back into the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting out the bigrams from the bigramList\n",
    "for i in range(len(bigramList)):\n",
    "    bigramWordList.append(bigramList[i][0])\n",
    "\n",
    "# Tokenizing the bigrams and using '__' as the separator between the bigrams\n",
    "mweTokenizer = MWETokenizer(bigramWordList, separator = '__')\n",
    "\n",
    "# Tokenizing the bigrams back into the dictionary\n",
    "for fileName, body in paperBodyDict.items():\n",
    "    paperBodyDict[fileName] = mweTokenizer.tokenize(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Removing stop words\n",
    "\n",
    "### 12.1. Removing context independent stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fileName, body in paperBodyDict.items():\n",
    "    body = [word for word in body if word not in stopWords]     # Removing stop words\n",
    "    paperBodyDict[fileName] = mweTokenizer.tokenize(body)       # Storing tokenized words with filename as it's key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2. Removing context dependent stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting upper and lower threshold values\n",
    "upperThreshold = (0.95)*len(paperBodyDict.keys()) \n",
    "lowerThreshold = (0.03)*len(paperBodyDict.keys()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we find out the frequency of each token in a pdf file, extract only the unique tokens and then append them into a list. After that we are count the frequency of the words in the list outside the loop as it will have the correct document frequency of all the words across all the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To record number of times a word has occurred per file (using FreqDist - nltk)\n",
    "for fileName, body in paperBodyDict.items():\n",
    "    bodyCount = FreqDist(body)\n",
    "    for key in bodyCount.keys():\n",
    "        tokenWordList.append(key)\n",
    "\n",
    "# Overall words frequency\n",
    "wordFrequency = FreqDist(tokenWordList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We capture only the words whose frequency falls between the lower and upper thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating vocabulary list of words whose frequency lies with the threshold specified earlier\n",
    "for key, value in wordFrequency.items():\n",
    "    if value <= upperThreshold and value > lowerThreshold:\n",
    "        vocabList.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary with words after removing stop words per file (filename as key)\n",
    "for fileName, body in paperBodyDict.items():\n",
    "    body = [word for word in body if word in vocabList]\n",
    "    dictWithoutStopWords[fileName] = mweTokenizer.tokenize(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Remove tokens with length less than 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary of words with length greater than or equal to 3 with filename as key\n",
    "for fileName, body in dictWithoutStopWords.items():\n",
    "    body = [word for word in body if len(word) >= 3]\n",
    "    dictWithoutSmallTokens[fileName] = body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of `Stemming` refers to reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. It is helpful in understanding Natural Language Processing (NLP). To perform `Stemming`, we are passing in a list of words as an argument to the function `stemWords` defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary with final set of words\n",
    "for fileName, body in dictWithoutSmallTokens.items():\n",
    "    body = stemWords(body)\n",
    "    finalDictionary[fileName] = body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Generating vocabulary text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fileName, body in finalDictionary.items():\n",
    "    bodyCount = FreqDist(body)                       # Taking frequency of each word per file\n",
    "    for key in bodyCount.keys():\n",
    "        vocabularyList.append(key)                   # Appending each word into vocabulary list \n",
    "vocabularyList = list(set(vocabularyList))           # Getting unique values only using set and converting it into list\n",
    "vocabularyList.sort()                                # Sorting the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing vocabulary information into a file\n",
    "\n",
    "vocabFile = open('Group010_vocab.txt', 'w', encoding='utf-8')\n",
    "vocabDict = {}\n",
    "\n",
    "for i in range(len(vocabularyList)):\n",
    "    vocabDict[vocabularyList[i]] = i\n",
    "    \n",
    "    # Writing into the file in required format\n",
    "    vocabFile.write(vocabularyList[i] + ':' + str(i) )\n",
    "    vocabFile.write('\\n')\n",
    "    \n",
    "vocabFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Generating sparse representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generting count vector file\n",
    "\n",
    "vectorFile = open('Group010_count_vectors.txt', 'w', encoding='utf-8')\n",
    "\n",
    "for fileName, body in finalDictionary.items():\n",
    "    sparseOutput = ''\n",
    "    vectorString = ''\n",
    "    bodyCount = FreqDist(body)\n",
    "    \n",
    "    # Storing the pattern in required format to write into the file\n",
    "    for element, value in bodyCount.items():\n",
    "        vectorString = vectorString + ',' + str(vocabDict[element]) + ':' + str(value)\n",
    "    \n",
    "    sparseOutput = fileName + vectorString\n",
    "    \n",
    "    # Writing into the file\n",
    "    vectorFile.write(sparseOutput)\n",
    "    vectorFile.write('\\n')\n",
    "    \n",
    "vectorFile.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: The sparse matrix generated for the tokens is in no particualr order as nothing is specified in the assignment requirement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Statistics generation\n",
    "\n",
    "### 17.1. Sorting author names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the author dictionary based on increasing name and decreasing count            \n",
    "authorList = sorted(authorDict.items(), key=lambda x: (-x[1], x[0]))  # Returns list of tuples(authors, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.2. Sorting title words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stripping leading and trailing spaces\n",
    "titles = titles.strip()\n",
    "\n",
    "# tokenize titles\n",
    "titleWords = tokenizer(titles)\n",
    "\n",
    "# Removing stop words from title words\n",
    "titleWords = [word for word in titleWords if word.lower() not in stopWords]        \n",
    "\n",
    "# Converting words into lower case\n",
    "titleWords = [element.lower() for element in titleWords]\n",
    "\n",
    "# Updating the title dictionary\n",
    "for word in titleWords:                                                 \n",
    "    if word in titleDict.keys():\n",
    "        titleDict[word] += 1                # If the word exists in the dict, increment value by 1\n",
    "    else:\n",
    "        titleDict[word] = 1                 # If the word doesn't exists in the dict, assign value as 1\n",
    "\n",
    "        \n",
    "# Sorting the title dictionary based on increasing word (based on letters) and decreasing count            \n",
    "title = sorted(titleDict.items(), key=lambda x: (-x[1], x[0]))  # Returns list of tuples(title words, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.3. Sorting abstract words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stripping leading and trailing spaces\n",
    "abstracts = abstracts.strip()\n",
    "\n",
    "# Substituting '-\\n' with nothing and '\\n' with space in abstract\n",
    "abstracts = re.sub('-\\n', '', abstracts)\n",
    "abstracts = re.sub('\\n', ' ', abstracts)\n",
    "\n",
    "# Sentence segmentation of abstracts\n",
    "sentenceDetector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentences = sentenceDetector.tokenize(abstracts)\n",
    "\n",
    "# Normalizing the sentence and storing it in form of a string\n",
    "combinedData = \"\"\n",
    "for sentence in sentences:\n",
    "    wordList = sentence.split(' ')\n",
    "    wordList[0] = wordList[0].lower()\n",
    "    combinedData += ' '.join(wordList)\n",
    "\n",
    "# Tokenize the abstracts\n",
    "combinedDataWords = tokenizer(combinedData)\n",
    "\n",
    "# Removing stop words from abstract words\n",
    "combinedDataWords = [word for word in combinedDataWords if word.lower() not in stopWords]\n",
    "\n",
    "\n",
    "# Updating the abstract dictionary\n",
    "for word in combinedDataWords:                                                 \n",
    "    if word in abstractDict.keys():\n",
    "        abstractDict[word] += 1                # If the word exists in the dict, increment value by 1\n",
    "    else:\n",
    "        abstractDict[word] = 1                 # If the word doesn't exists in the dict, assign value as 1\n",
    "        \n",
    "# Sorting the abstract dictionary based on increasing word (based on letter) and decreasing count            \n",
    "abstract = sorted(abstractDict.items(), key=lambda x: (-x[1], x[0]))  # Returns list of tuples(abstract words, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.4. Creating a dataframe and storing top 10 abstract words, title words and author names\n",
    "\n",
    "<br>Here, we have created a dataframe (using pandas) that stores top 10 abstracts, titles and authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe with three columns\n",
    "df = pd.DataFrame(columns=['top10_terms_in_abstracts','top10_terms_in_titles','top10_authors'])\n",
    "\n",
    "# Storing top10 abstracts, titles and author names into dataframe\n",
    "for i in range(0, 10):\n",
    "    df = df.append({'top10_terms_in_abstracts': abstract[i][0], 'top10_terms_in_titles': title[i][0], 'top10_authors': authorList[i][0]}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.5. Exporting statistics to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting dataframe content to a csv file \n",
    "df.to_csv('Group010_stats.csv', index = False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize\n",
    "\n",
    "https://stackoverflow.com/questions/26494211/extracting-text-from-a-pdf-file-using-pdfminer-in-python\n",
    "\n",
    "http://www.blog.pythonlibrary.org/2018/05/03/exporting-data-from-pdfs-with-python/\n",
    "\n",
    "https://stackoverflow.com/questions/44699682/how-to-save-a-file-downloaded-from-requests-to-another-directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
